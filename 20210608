강화학습 actor-critic
정책함수와 가치함수가 분리되어 있다. -> 함수가 2개라는 것 
정책함수 = actor 가치함수 = critic ; 정책에 따른 행위의 결과(얼마나 worth it한지)를 평가(evaluate)하기 때문에 "critic
actor : 상태가 주어졌을 떄 행동을 결정한다 -> 파이
critic : 가치를 평가한다 V(s)
Actor-critic simulation
A2C(Advantage Actor-Critic)
  카트 폴 시스템(cart-pole system)의 균형을 위해 actor-critic(AC)에이전트 훈련
  A2CActor-Critic + Advantage(Actor의 expected output) => A2C(Advantage Actor-Critic)
  단점 : DQN처럼 replay buffer를 활용하지 않고 탐색 데이터를 <즉시(immediately)> 학습에 이용하기 때문에 잘못된 길로 학습했을 경우 결과값이 안좋을 수 있다.
A3C(Asynchronous + A2C = Asynchronous Advantage Actor-Critic) *A3C는 asynchronous하게 동작함
  DQN같은 알고리즘은 하나의 에이전트가 최적의 정책을 학습한다. 반면 A3C는 환경과 상호작용하는 여러 에이전트가 존재한다
  한 개가 아닌 여러 개의 에이전트를 실행하여 주기적, 비동기전(asynchronou)으로 공유 네트워크를 업데이트한다
  다른 에이전트의 실행과는 독립적으로 각각의 에이전트가 공유네트워크를 업데이트하고 싶을 때 업데이트 = asynchronization
  cf) 바로 학습해버리는 DQN

actor-critic방법은 가치함수와 독립적인 정책함수를 나타내는 temporal difference(TD)학습방법
what is TD? 시간차학습
